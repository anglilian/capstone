{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from random import randint\n",
    "from time import sleep \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class en_webscraper():\n",
    "    \"\"\"\n",
    "    Extracts English article data from GoogleNews\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    search_term : str\n",
    "        Term to search in Google\n",
    "    start_date : str\n",
    "        Date of earliest article in format MM/DD/YYYY\n",
    "    end_date : str\n",
    "        Date of latest article in format MM/DD/YYYY (default today) \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, search_term, start_date, end_date = datetime.today().strftime('%d/%m/%Y')):\n",
    "        self.df = pd.DataFrame(columns=['title', 'media', 'google_date', \"converted_date\", 'link', 'article'])\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date \n",
    "        self.search_term = search_term\n",
    "\n",
    "        # Set up your user agent for making HTTP requests\n",
    "        user_agent = \"INSERT HERE\" #https://www.whatismybrowser.com/detect/what-is-my-user-agent \n",
    "        self.config = Config()\n",
    "        self.config.browser_user_agent = user_agent\n",
    "\n",
    "        self.lang = \"en\"\n",
    "        self.start_date = \"\".join(start_date.split(\"/\"))\n",
    "        self.end_date = \"\".join(end_date.split(\"/\"))\n",
    "\n",
    "    def search_google(self):\n",
    "        \"\"\"\"Retrieves all the search results from Google page by page\"\"\"\n",
    "\n",
    "        googlenews=GoogleNews(start=self.start_date, end=self.end_date)\n",
    "        googlenews.search(self.search_term)\n",
    "        for i in range(30):\n",
    "            result= googlenews.page_at(i+1)\n",
    "            if not result:\n",
    "                break\n",
    "            else:\n",
    "                self.df = self.df.append(result) \n",
    "            sleep(randint(10,30))\n",
    "\n",
    "        self.df = self.df.set_index(np.arange(0, len(self.df)))\n",
    "        self.df = self.df.drop(columns = [\"desc\", \"img\", \"datetime\"])\n",
    "        self.df[\"lang\"] = \"EN\"\n",
    "\n",
    "        self.get_articles()\n",
    "        \n",
    "    def string_to_date(self, s):\n",
    "        \"\"\"\n",
    "        Converts string to date time objects\n",
    "\n",
    "        Input: \n",
    "        s (str): Date as string\n",
    "        \"\"\"\n",
    "\n",
    "        if \"Sept\" in s:\n",
    "                s = s.replace(\"Sept\", \"Sep\")\n",
    "        if \"ago\" in s:\n",
    "            parsed_s = [s.split()[:2]]\n",
    "            if parsed_s[0][-1][-1] != \"s\":\n",
    "                parsed_s[0][-1] += \"s\"\n",
    "            time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\n",
    "            dt = relativedelta(**time_dict)\n",
    "            date = datetime.now() - dt\n",
    "            return date.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            return datetime.strptime(s, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "    def find_date(self, url):\n",
    "        \"\"\"Searches metadata of url to find the publishing date\"\"\"\n",
    "\n",
    "        r= requests.get(url)\n",
    "        soup =BeautifulSoup(r.text, \"html.parser\")\n",
    "        data = soup.findAll('script')\n",
    "        try:\n",
    "            date = regex.search(r'\\d{4}-\\d{2}-\\d{2}', str(data))[0]\n",
    "        except:\n",
    "            date = np.nan\n",
    "        return date\n",
    "\n",
    "    def get_articles(self):\n",
    "        \"\"\"Extracts the article text and date\"\"\"\n",
    "\n",
    "        for i in range(len(self.df)):\n",
    "            article = Article(self.df[\"link\"][i], config=self.config)\n",
    "            article.download()\n",
    "            try:\n",
    "                article.parse()\n",
    "                self.df[\"article\"][i] = article.text.replace(\"\\n\", \" \")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if \"month\" in self.df.date[i] or \"week\" in self.df.date[i]:\n",
    "                self.df.converted_date[i] = self.find_date(self.df.link[i])\n",
    "            else:\n",
    "                self.df.converted_date[i] = self.string_to_date(self.df.date[i])\n",
    "\n",
    "        self.export_df()\n",
    "\n",
    "    def export_df(self):\n",
    "        \"\"\"Saves scraped article data as a csv and pkl file\"\"\"\n",
    "        \n",
    "        self.df.to_csv(f'{self.lang}_articles_{\"_\".join(self.search_term.split(\" \"))}_{self.start_date}_{self.end_date}.csv', \n",
    "            index = False)\n",
    "        self.df.to_csv(f'{self.lang}_articles_{\"_\".join(self.search_term.split(\" \"))}_{self.start_date}_{self.end_date}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "# Run these lines of code\n",
    "eng = en_webscraper(\"vernacular school malaysia\", \"1/1/2015\", \"12/31/2015\")\n",
    "eng.search_google()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bm_webscraper(en_webscraper):\n",
    "    \"\"\"\n",
    "    Extracts Malay article data from Google\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    search_term : str\n",
    "        Term to search in Google\n",
    "    start_date : str\n",
    "        Date of earliest article in format MM/DD/YYYY\n",
    "    end_date : str\n",
    "        Date of latest article in format MM/DD/YYYY (default today) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, search_term, start_date, end_date):\n",
    "        super().__init__(search_term, start_date, end_date)\n",
    "        # Set up selenium webdriver\n",
    "        self.PATH = \"C:\\Program Files (x86)\\chromedriver.exe\" # Insert path to chromedriver\n",
    "        self.options = Options()\n",
    "        self.options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "        search = \"+\".join(search_term.split(\" \")) \n",
    "        start = \"%2F\".join(start_date.split(\"/\")) \n",
    "        end = \"%2F\".join(end_date.split(\"/\")) \n",
    "        self.URL = f'https://google.com/search?q={search}&tbm=nws&start=0&num=200&tbs=cdr%3A1%2Ccd_min%3A{start}%2Ccd_max%3A{end}'\n",
    "\n",
    "        self.lang = 'bm'\n",
    "        self.open_google()\n",
    "        \n",
    "    def open_google(self):\n",
    "        \"\"\"Open the Google browser first to click out of pop-ups\"\"\"\n",
    "\n",
    "        self.driver = webdriver.Chrome(self.PATH, options = self.options)\n",
    "        self.driver.get(self.URL)\n",
    "\n",
    "    def search_google(self):\n",
    "        \"\"\"Go through all pages of search results and store relevant info\"\"\"\n",
    "\n",
    "        # Get the page navigation bar element\n",
    "        page_navigation = self.driver.find_element_by_class_name(\"AaVjTc\")\n",
    "        pages = page_navigation.find_elements_by_css_selector(\"td\")\n",
    "        if pages:\n",
    "            page_numbers = [i for i, pg in enumerate(pages) if pg.text.isnumeric()]\n",
    "        else:\n",
    "            page_numbers = [1]\n",
    "\n",
    "        # Get the links of all the pages of search results\n",
    "        soup = []\n",
    "        for pg in page_numbers:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"main\")))\n",
    "            soup.append(BeautifulSoup(self.driver.page_source, \"lxml\"))\n",
    "            if pg+1 <= len(page_numbers):\n",
    "                pages[pg+1].click()\n",
    "                page_navigation = self.driver.find_element_by_class_name(\"AaVjTc\")\n",
    "                pages = page_navigation.find_elements_by_css_selector(\"td\")\n",
    "\n",
    "        # Click through each page of search results\n",
    "        for s in soup:\n",
    "            link = []\n",
    "\n",
    "            # Extract all the media article links on each page\n",
    "            for item in s.find_all(\"a\", class_=\"WlydOe\"):\n",
    "                link.append(item.get('href'))\n",
    "\n",
    "            # Extract each media article's data        \n",
    "            google_date = [item.getText() for item in s.find_all(\"p\", class_=\"S1FAPd\")]\n",
    "            converted_date = [string for string in google_date]\n",
    "\n",
    "            self.df = pd.concat([self.df, \n",
    "                pd.DataFrame({\n",
    "                \"title\": [item.getText() for item in s.find_all('div', class_ = \"mCBkyc JQe2Ld nDgy9d\")],\n",
    "                \"media\": [item.getText() for item in s.find_all(\"div\", class_=\"CEMjEf\")],\n",
    "                \"google_date\": google_date,\n",
    "                \"converted_date\": converted_date,\n",
    "                #\"page_date\": [find_date(l) for l in link],\n",
    "                \"link\": link,\n",
    "                \"article\": \"\",\n",
    "                \"lang\": \"BM\"\n",
    "            })], ignore_index = True)\n",
    "\n",
    "        self.driver.quit()\n",
    "        self.get_articles()\n",
    "\n",
    "    def get_articles(self):\n",
    "        \"\"\"Extract article text from each URL\"\"\"\n",
    "        \n",
    "        for i in self.df.index:\n",
    "            self.driver = webdriver.Chrome(self.PATH, options = self.options)\n",
    "            self.driver.set_page_load_timeout(20) # Terminate if page takes too long to load\n",
    "            try:\n",
    "                self.driver.get(self.df.link[i])\n",
    "            except:\n",
    "                self.driver.execute_script(\"window.stop();\")\n",
    "            \n",
    "            # Find article text by p tags\n",
    "            try:\n",
    "                article = WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"p\"))\n",
    "                )\n",
    "                article = self.driver.find_elements_by_css_selector('p')\n",
    "                text = [item.text for item in article]\n",
    "                self.df.article[i] = \" \".join(text)\n",
    "            except:\n",
    "                raise \n",
    "            \n",
    "            self.driver.quit()\n",
    "        \n",
    "        self.export_df() # Save results as csv and pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line first and click out of any pop ups from Google\n",
    "bm = bm_webscraper(\"sekolah vernakular malaysia\", \"1/1/2017\", \"12/31/2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then run this\n",
    "bm.search_google()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(folder):\n",
    "    \"\"\"Combine csv files into one file\"\"\"\n",
    "    all_filenames = [f for f in os.listdir(folder) if f.endswith(\"csv\")]\n",
    "    combined_csv = pd.concat([pd.read_csv(folder+f) for f in all_filenames])\n",
    "    combined_csv.to_csv(\"scraped_articles.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae9e5fb05c4e07405c95453073fad91cde291ec1571923ebb60ab806dff8b1c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
